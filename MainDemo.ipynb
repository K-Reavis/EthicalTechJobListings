{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str(tup):\n",
    "    if tup[0] == '':\n",
    "        return tup[1]\n",
    "    return tup[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_files(companies, titles, texts, start_point):\n",
    "    end_point = len(companies) - 1\n",
    "    for i in range(start_point, end_point):\n",
    "        post_id = 'postings/' + str(i) + '-' + re.sub(\"[^A-Za-z]\", \"\", companies[i].strip()) + '-' + re.sub(\"[^A-Za-z]\", \"\", titles[i].strip()) + '.rtf'\n",
    "        out_file = open(post_id,'w', encoding=\"utf-8\", errors = 'ignore')\n",
    "        out_file.write(texts[i])\n",
    "        out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_indeed(companies, titles, urls):\n",
    "    text_lst = []\n",
    "    for url in urls:\n",
    "        myRequest = requests.get(url)\n",
    "        soup = str(BeautifulSoup(myRequest.text, \"html.parser\")).replace(\"\\n\", \"\")\n",
    "        paragraph_text = r'<p>(.*?)</p>'\n",
    "        list_text = r'<li>(.*?)</li>'\n",
    "        clean = re.compile('<.*?>')\n",
    "        text = \" \".join([re.sub(clean, '', get_str(x)).replace(\"amp;\", \"\") for x in re.findall(paragraph_text + \"|\" + list_text, soup)])\n",
    "        text_lst.append(text)\n",
    "    return text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_terms_indeed(df, terms):\n",
    "    for term in terms:\n",
    "        url = \"https://www.indeed.com/jobs?q=\" + term.replace(\" \", \"+\") + \"&fromage=1\"\n",
    "        myRequest = requests.get(url)\n",
    "        soup = str(BeautifulSoup(myRequest.text, \"html.parser\"))\n",
    "\n",
    "        title_match = 'rel=\"noopener nofollow\" target=\"_blank\" title=\"([^\"]+)'\n",
    "        titles = [x.replace(\"amp;\", \"\") for x in re.findall(title_match, soup)]\n",
    "\n",
    "        match_1 = r'<span class=\"company\">\\n([^<]+)'\n",
    "        match_2 = r'rel=\"noopener\" target=\"_blank\">\\n([^<]+)'\n",
    "\n",
    "        companies = [get_str(x).replace(\"amp;\", \"\") for x in re.findall(match_1 + '|' + match_2, soup)]\n",
    "\n",
    "        url_match = r'data-tn-element=\"jobTitle\" href=\"([^\"]+)' \n",
    "        urls = [\"https://www.indeed.com\" + x.replace(\"amp;\", \"\") for x in re.findall(url_match, soup)]\n",
    "\n",
    "        texts = get_text_indeed(companies, titles, urls)\n",
    "        \n",
    "        location_match = r'data-rc-loc=\"([^\"]+)'\n",
    "        locations = re.findall(location_match, soup)\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "\n",
    "        new_df['Platform'] = [\"Indeed\"] * len(companies)\n",
    "        new_df['Search_Term'] = [term] * len(companies)\n",
    "        new_df['Title'] = titles\n",
    "        new_df['Company'] = companies\n",
    "        new_df['Location'] = locations\n",
    "        new_df['URL'] = urls\n",
    "        new_df['Text'] = texts\n",
    "        new_df['Date_Queried'] = [datetime.now().date()] * len(companies)\n",
    "\n",
    "        df = df.append(new_df).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_linkedin(companies, titles, urls):\n",
    "    text_lst = []\n",
    "    for url in urls:\n",
    "        myRequest = requests.get(url)\n",
    "        soup = str(BeautifulSoup(myRequest.text, \"html.parser\")).replace(\"\\n\", \"\")\n",
    "        match = r'<br/>(.*?)<br/>'  \n",
    "        clean = re.compile('<.*?>')\n",
    "        text = \" \".join([re.sub(clean, '', x).replace(\"amp;\", \"\") for x in re.findall(match, soup)])\n",
    "        text_lst.append(text)\n",
    "    return text_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_terms_linkedin(df, terms):\n",
    "    for term in terms:\n",
    "        url =(\"https://www.linkedin.com/jobs/search?keywords=\" + term.replace(\" \", \"%20\") + \n",
    "              \"&location=United%20States&trk=public_jobs_jobs-search-bar_search-submit&redirect=false&position=1&pageNum=0&f_TP=1\")\n",
    "        myRequest = requests.get(url)\n",
    "        soup = str(BeautifulSoup(myRequest.text, \"html.parser\"))\n",
    "\n",
    "        title_match = r'<span class=\"screen-reader-text\">([^<]+)'\n",
    "        titles = [x.replace(\"amp;\", \"\") for x in re.findall(title_match, soup)]\n",
    "\n",
    "        company_match = r'alt=\"([^\"]+)'\n",
    "        companies = [x.replace(\"amp;\", \"\") for x in re.findall(company_match, soup)][:-1]\n",
    "\n",
    "        url_match = r'href=\"https://www.linkedin.com/jobs/view([^\"]+)' \n",
    "        urls = [\"https://www.linkedin.com/jobs/view\" + x for x in re.findall(url_match, soup)]\n",
    "\n",
    "        location_match = r'<span class=\"job-result-card__location\">([^<]+)'\n",
    "        locations = re.findall(location_match, soup)\n",
    "\n",
    "        new_df = pd.DataFrame()\n",
    "        new_df['Platform'] = [\"LinkedIn\"] * len(companies)\n",
    "        new_df['Search_Term'] = [term] * len(companies)\n",
    "        new_df['Title'] = titles\n",
    "        new_df['Company'] = companies\n",
    "        new_df['Location'] = locations\n",
    "        new_df['URL'] = urls\n",
    "        new_df['Date_Queried'] = [datetime.now().date()] * len(companies)\n",
    "\n",
    "        df = df.append(new_df).drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_terms = [\"Trust\", \"Privacy\", \"Diversity\", \"Equity\", \n",
    "\"Equality\", \"Inclusion\", \"Ethics\",\"Policy\", \"Educational\", \n",
    "\"Compliance\",\"Emerging\",\"Responsible\", \"Accountability\"]\n",
    "\n",
    "search_terms = [x + \" Technology\" for x in key_terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New DataFrame code below\n",
    "#df = pd.DataFrame(columns = ['Platform','Search_Term','Title','Company','Location','URL', 'Text','Date_Queried'])\n",
    "#start_point = 0\n",
    "\n",
    "df = pd.read_csv(\"postings.csv\")\n",
    "start_point = df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = add_terms_linkedin(df, search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyler\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "df = add_terms_indeed(df, search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['URL']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_files(df[\"Company\"], df[\"Title\"], df[\"Text\"], start_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"postings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
